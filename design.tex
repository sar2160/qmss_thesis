\documentclass{article}
\usepackage{setspace}
\usepackage{\todonotes}
\usepackage{fullpage}


\doublespacing

\section{TO DO}
\listof\todos

\section{Research Design}

\subsection{Data}

This project will use  observational administrative data gathered as part of New York City's 311 civic reporting system. The 311 system is both a customer-facing tool for New York City residents to submit requests for city services and a system-of-record for tracking outcomes of these requests. This study will focus on service requests that are both relatively prevalent across the entire city and which provoke an unambigious remediative action from city government. \par

Complaints about the presence of rodents are incredibly common in New York City and pose a real public health risk which government has an interest in proactively addressing. Public complaints are
also a primary source for information about ongoing rodent problems where New York's Department of Health and Mental Hygiene (DOHMH) is tasked with a response. While being a common occurence
on private and public land in New York, rodent problems are also quite sparse when viewed spatially. Rodent presence may be confined to a single city block or even a single building
while neighboring structures and areas will be unaffected, although possibly at higher risk of developing a problem due to their proximity. This relative sparsity makes
accurate forecasting at fine spatial granularity potentially much more useful than the perspective a high-level ecological survey might take. Even the neighborhood or
precinct level approach used for predictive policing covers an area larger than would be useful for proactively identifying and responding to rodent infestations. The average
New York City police precinct comprises \todo{Z, \i fill-in} census blocks. \par

The data used in this project will cover the period of 2016 and be aggregated to a count of rodent complaints received over a given period of time within a census block. Although
the same data are available for several years prior to 2016 and are available in real-time presently , using 2016 data is also advantageous because NYC DOHMH carried out a separate
and comprehensive 'rat-indexing' program in certain neighborhoods during 2016. This program mandated complete proactive rodent inspection and mitigation of designated neighborhoods
regardless of incoming 311 complaints. The rat-indexing program offers an independently sourced estimate for eventual comparison with the level of rodent nuisance forecasted
using only complaint-based data.

\subsection{Models}

This project will assess the viability of using Bayesian Gaussian Process models for spatiotemporal predictions in urban settings. Gaussian Processes have attractive properties for sparse data because a model can be interpreted as interpolating an unobserved point between known distributions \todo{Also, not totally confident in this statement}. As mentioned in the literature review, spatiotemporal models have historically suffered from computational infeasibility when the amount of data as well as spatial and temporal dimensionality grows. Recent advances in Bayesian probablistic programming (Stan) and implementations of Gaussian Processes that take advantage of the relative sparsity of data across spatial dimensions has reduced the computational burden of fitting these models, although the spatial granularity proposed here will require a great deal of posterior checks to ensure the model is fitting correctly. \par

A Gaussian Process model will be compared to various existing spatial, temporal, and spatiotemporal modeling methods for this type of forecasting problem. Standard maximum-likelihood autoregressive models like ARIMA will be considered, as well as existing Bayesian methodologies like INLA. Out-of-sample predictive accuracy will be assessed with a root-mean-squared-error (RMSE) metric that
can be easily applied to all models. Robustness of forecasting results and model sensitivity to tuning parameters will also be assessed with the goal of providing insight into the practical viability of running models in a production environment. The relative speed of fitting different models will also be a factor for assessing viability. \par


\subsection{The Log-Gaussian Cox Process}

Log-Gaussian Cox Process (LGCP) models are a further extension of a Gaussian Process with particular applicability to prediction of count data. A LGCP is hierarchical in that the data are assumed to be drawn from a Poisson likelihood with intensity parameter $\lambda$. The Poisson likelihood function has theoretical properties suitable for relatively sparse count (integer) data which makes it appealing for use in modelling frequent events that are nevertheless sparse when segmented over space and time dimensions. In turn the log of $\lambda$  is generated by a Guassian process\cite{teng_2017}. This makes the LGCP quite flexible in inputs and dimensionality while also making model fitting generally computationally burdensome. \par

An alternative specification used a Gaussian likelihood, which has the added appeal of making the likelihood and prior conjugate to each other and solvable in closed form. As the $\lambda$ parameter of a Poisson distribution increases the distribution converges to a Normal distribution anyway, so this model may be more applicable to situations when counts are not sparse e.g., for at lower granularities of space and/or time, which was not considered in detail here \todo{ADD SOME DATA for normal model?}

 The general model follows the specification and notation used by "A General Approach to Prediction and Forecasting Crime Rates with Gaussian Processes" \cite{flaxman_2014}: \par


$$\lambda(s,t) = exp((s,t))$$

$$ y_{s,t} | \lambda(s,t) ~ Poisson(exp(f(s,t))e_t) $$

The outcome count $y_{s,t}$ at location $s$ and time $t$ is generated from a Poisson distribution with parameter $f(s,t)e_t$. $f(s,t)$ is a function with a Gaussian process prior, while $e_s$ is a fixed spatial expectation term $\mathbb{E}[y_s]$. The spatial expectation is a convenient way to incorporate prior information about the variable of interest. Again following Flaxman 2014, using an exponential link function gives a practical interpretation of $exp(f(s,t))$ as the relative risk function while $f(s,t)$ itself is the log-relative risk. When the log-relative risk is 0, the relative risk is 1 and the expected value of of the outcome $y(s,t)$ is the just the prior spatial expectation $e_s$. Finally, every count outcome $y_{s,t}$ is assumed independent conditional on $f$, so the joint conditional likelihood of all $y$ factors as a product.

$$p(y|f) = \prod{ Poisson(exp(f(s,t))e_t)}$$

\subsubsection{f(s,t)}

$f$ is modeled as following a generic Gaussian process with a mean of zero and a covariance matrix $K$:

$$ f ~ GP(0,K) $$

Spatiotemporal elements enter the Gaussian process model through $K$, which is used to capture the relationships of interest and actually entirely determines the model. Since variance/covariance is additive, any variance term can be decomposed at least theoretically into arbitrarily many additive components. Purely spatial and temporal elements as well as interactions can be incorporated using different kernel functions, and different combinations of kernels may produce more accurate results. Cross-validating models with different types and combinations of kernel functions will help identify the specification with the best predictive properties. Since the cross-validation data are correlated in this model the cross-validation set will have to be drawn from spatially contigous representative subsets of the data.

\subsubsection{Kernels}

There are a wide variety of existing kernel functions documented for use in Gaussian process models \cite{rasmussen_2005} and it is also possible to define custom functions as long as they follow certain properties. Since the entire model is determined by its kernels there is a lot of room for experimentation and choice. The models considered for this project loosely follow Flaxman 2014 by consisting of up to four individual kernels $k$ , where $K=\sum{i}k_i$. The four basic kernel types were:

\begin{itemize}
  \item $k_s(s)$ : a spatial kernel
  \item $k_t(t)$ : a temporal kernel
  \item $k_p(t)$ : a periodic kernel
  \item $k_{st}(s,t)$ : a periodic kernel
\end{itemize}

The kernel types specified for each $k$ differed slightly from Flaxman after experimenting with various configurations.

\todo{MORE DETAILS ON CHOICE HERE}


\subsubsection{Methods for fitting LGCPs}

Solutions to the computational challenges in fitting LGCPs have advanced rapidly over the past five years. As recently as 2012 the best methods for fitting LGCP models involved variations of the Metropolis-Hastings sampling algorithm which were considered to be slow and highly inefficient in generating acceptable draws from the posterior distribution of the model \cite{murray_2012}. Advances in statistical computing have opened the door for several new model fitting methods, each with their own advantages and drawbacks.

\subsubsection{Variational Inference}

Variational Inference (VI) methods attempt to recover parameter estimates for a posterior distribution of interest by specifying a more tractable family of distributions and optimizing the resulting approximation using closed-form or computational methods. In the case of the LGCP there is no direct closed-form solution, but advances in computation - in particular autodifferentiation algorithms - have made the process easier. However, there are no theoretical justifications for the accuracy of estimates produced by VI because it is unclear and often unknowable how close the optimized approximation is to the true posterior distribution (Kullback-Leibler divergence). Yao et., al. recently proposed several diagnostic methods for assessing whether a VI approximation actually 'worked' \cite{yao_2018}.

\subsubsection{MCMC}

Markov Chain Monte Carlo (MCMC) sampling methods by contrast do have theoretical properties that ensure consistent estimation of the posterior distribution - at least  as the numbers of samples increases asympotically \cite{teng_2017}. MCMC methods are not a new development in themselves, but there have been breakthroughs both in MCMC algorithm design and computing power required to fit more complex models. Stan has been used for spatiotemporal modelling of causes of mortality using Gaussian Markov Random Field models, another potential alternative to LGCPs that may be appropriate for urban forecasting but are not considered here \cite{foreman_2017}. In contrast to VI, MCMC also is capable of producing estimates for full posterior distributions for parameters of interest, rather than point estimate approximations. In the case of urban prediction and forecasting, access to full posterior estimates would offer much more probalistic information from which to draw uncertainty-based conclusions in a policy or administrative context \todo{EXAMPLE HERE}.

\subsubsection{VFF}
\todo{MAYBE MOVE TO DISCUSSION}
see GPML pg. 82 , Bohner's theorem


\subsubsection{Method Choice}

While MCMC methods clearly offer desirable properties superior to VI methods, their practical limitations made them somewhat burdensome to consider for this project. The most unfortunate drawbacks were that MCMC is very slow to fit in comparison with VI and that specifying tractable LGCP models in an MCMC language such as Stan requires writing more complex and less flexible code \cite{Flaxman2015FastHG}. In an attempt to draw a compromise between ease of experimentation and model reliability this project used primarily VI methods under the knowledge that these methods may not produce the best results for practical use.



\subsection{Model Specification}




\end{document}
